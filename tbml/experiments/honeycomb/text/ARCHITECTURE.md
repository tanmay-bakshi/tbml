## Base Model (train.py)

This section documents the text "base model" trained by `tbml/experiments/honeycomb/text/train.py`.
It summarizes the data format, preprocessing, model architecture, losses, and optimization as
implemented in the current code.

### Data format and batching

- Training data is pre-tokenized and stored as fixed-length NumPy shards (`shard_*.npy`) with
  shape `(num_samples, max_seq_len)`. The dataset is loaded via `MMapTokenDataset`, which memory
  maps shards and provides random access.
- Each sample is already padded to `max_seq_len` with a padding token. The dataset metadata
  includes `vocab_size`, `max_seq_len`, and the special token ids (`eos_id`, `pad_id`, `mask_id`).
- Batches are produced by `iter_text_batches`, which performs a streaming shuffle using a
  bounded buffer. The training script reshapes each host batch into per-device slices and
  prefetches to device.
- EOS tokens are stripped before the model forward pass by replacing EOS ids with the padding
  id. This means the model never attends to EOS positions during training.

### View generation and masking

- LeJEPA-style training uses multiple masked "views" per sample: a configurable number of
  **global** and **local** views, plus a single unmasked **sample view**.
- Each view is generated by BART-style span masking:
  - A target masking ratio is sampled uniformly between a per-view min/max (defaults: global
    25–30%, local 40–45%).
  - Span lengths are sampled from a Poisson distribution (lambda = 3).
  - Spans are chosen from non-pad, non-EOS positions.
  - At least one unmasked token is guaranteed per view.
- The masked spans are **not removed** and tokens are **not replaced** with a mask token in the
  base model by default. Instead, masking is enforced via the attention mask:
  - Masked positions are excluded as keys/values in attention.
  - Outputs at masked positions are zeroed by the attention implementation.
  - Positional indices are preserved, so the model can still infer relative distance.
  - If `--mask-token-input` is enabled, masked positions are instead replaced with the mask token
    id for the encoder input, and the encoder attention mask treats them as visible tokens.
- The sample view uses the full (unmasked) sequence; it is not used in the LeJEPA loss directly,
  but is used as a target for the predictor loss.

### Model architecture

The base model is `TextTransformer` (see `tbml/experiments/honeycomb/text/model.py`). It is
composed of an **encoder**, a **predictor**, and a **decoder**.

- **Token embedding**
  - Embedding table of shape `(vocab_size, d_model)`.
  - Embedding normalization is configurable in the model config; current training sets
    `embed_norm=False` (no post-lookup RMSNorm).
  - The embedding module exposes an `unembed()` method that can project embeddings back to
    vocabulary logits using the raw embedding weights.
- **Transformer blocks**
  - Pre-norm RMSNorm.
  - Self-attention with PoPE or RoPE positional encoding (configured via `attn_type`).
  - Attention is **causal** for both encoder and predictor when `--causal-attention true`, and
    **non‑causal** when set to false. Attention also respects the provided attention masks
    (pads and masked spans).
  - MLP: SwiGLU with expansion ratio `mlp_ratio`.
  - DropPath is applied with a linear schedule over depth.
- **Final normalization**
  - A final RMSNorm is applied to all token representations.
  - The encoder outputs are then passed through an additional SwiGLU feed‑forward layer
    before being used downstream.
- **Pooling**
  - The pooled sequence embedding is the representation of the **last valid position** according
    to the attention mask (i.e., the last non-masked, non-pad token).
- **Predictor**
  - A separate transformer stack that operates on token‑level representations after the
    encoder’s final norm. Causality follows the `--causal-attention` setting.
  - Number of predictor layers is configurable independently (`predictor_n_layers`); other
    hyperparameters (width, heads, MLP ratio, dropout, positional encoding) mirror the encoder.
  - Each masked token position is replaced by a learned position‑specific embedding before
    entering the predictor, unless `--mask-token-input` is enabled (in which case the encoder
    outputs for all positions are passed through directly).
  - The predictor outputs token representations **prior** to its own final RMSNorm; the
    final norm is applied only when forming the predictor reconstruction targets.
- **Decoder**
  - A transformer decoder stack trained to autoregressively reconstruct masked spans.
  - Each decoder block interleaves causal RoPE/PoPE self‑attention and non‑causal cross‑attention
    (no positional encoding) over predictor outputs for the masked span.
  - The decoder consumes token embeddings that are tied to the encoder’s embedding table, and
    its output logits are produced via the same shared embedding weights (`TokenEmbedding.unembed`).
  - Predictor and decoder are optional: if `predictor_n_layers` or `decoder_n_layers` is set to
    0, that component is not instantiated and any loss depending on it must be disabled.

The model returns both per-token representations and the pooled representation on every call.

### Losses

Let `B` be batch size, `V` be total number of views, and `K` the embedding dimension.
The model produces a tensor of pooled view embeddings with shape `(B, V, K)`.

1) **Sequence reconstruction loss (`seq_rec_loss`)**

- For each sample, the **global center** is computed as:
  - take the pooled *pre‑final‑norm* representations of the global views,
  - average them,
  - then apply the base model’s final RMSNorm to that average.
- All views are compared to this center.
- The prediction loss is either:
  - Mean squared error (MSE), or
  - Cosine distance (1 − cosine similarity).

2) **Sequence SIGReg (`seq_sigreg_loss`)**

- A regularization term computed from random projections of the view embeddings
  (see `tbml/experiments/honeycomb/loss.py`).
- The random directions are synchronized across devices via the global step and seed.
- SIGReg is computed on the **post‑final‑norm** pooled view embeddings.

3) **Sequence LeJEPA loss (`seq_lejepa_loss`)**

```
seq_lejepa_loss = (1 - sigreg_weight) * seq_rec_loss + sigreg_weight * seq_sigreg_loss
```

4) **Span reconstruction loss (`span_rec_loss`)**

- By default, for each global/local view, every masked span is reconstructed by comparing:
  - the base model’s *pre‑final‑norm* sample view representations averaged over that span
    (then passed through the base model’s final RMSNorm), against
  - the predictor’s output for the same span averaged and passed through the predictor’s
    final RMSNorm.
- If `--span-tokenwise` is enabled, the loss is computed **per token** within each masked span
  (after applying the final norms), and then averaged across tokens, spans, and views.

5) **Span SIGReg (`span_sigreg_loss`)**

- One random masked‑span target per sample is selected for **each view** (global and local).
- These per‑view targets are stacked into a `(B, V, D)` tensor and passed to a single
  SIGReg call, matching the sequence‑level SIGReg structure.

6) **Span LeJEPA loss (`span_lejepa_loss`)**

```
span_lejepa_loss = (1 - sigreg_weight) * span_rec_loss + sigreg_weight * span_sigreg_loss
```

7) **Decoder loss (`decoder_loss`)**

```
decoder_loss = cross_entropy(decoder_logits, decoder_targets)
```

Each view contributes **one randomly selected masked span** as an independent decoder sample:

- Input sequence: `[BOS] + span_tokens`
- Target sequence: `span_tokens + [EOS]`
- Cross‑attention memory: predictor outputs for the masked span positions.
- Loss is masked to valid (non‑pad) positions only.

8) **Total loss (`total_loss`)**

```
total_loss = seq_frac * seq_lejepa_loss
           + span_frac * span_lejepa_loss
           + decoder_frac * decoder_loss
           + mlm_frac * encoder_mlm_loss
```

The fractions are derived from the CLI flags `--seq-loss-weight`, `--span-loss-weight`,
`--decoder-loss-weight`, and `--encoder-mlm-loss-weight` by normalizing them to sum to 1.0.
When any weight is 0, its loss is skipped entirely.

9) **Encoder MLM loss (`encoder_mlm_loss`)**

- For every masked position in every global/local view, the encoder’s **post‑final‑norm**
  token representation is unembedded through the tied token embedding weights.
- Cross‑entropy is computed against the original token id, and top‑1/top‑5 accuracy are
  reported for masked positions only.

### Optimization and training loop

- Optimizer: `MuonWithAdamWFallback`.
  - Muon is applied to most parameters.
  - AdamW is used for excluded parameter groups (token embeddings, norms, and PoPE delta).
- Gradient accumulation is supported; micro-batches are stacked and reduced on device.
- Training uses `jax.pmap` across devices, with gradients and losses averaged across the
  `"data"` axis.
- The training loop performs a single full pass through the dataset (one epoch), unless
  `--max-train-steps` truncates it.
- Checkpoints are saved every `--checkpoint-every` steps and include model weights,
  optimizer state, and the run config.

### Precision

- Compute dtype is configurable (`float32`, `bfloat16`, `float16`).
- When using reduced precision, model parameters are stored in `float32`.
- Embeddings are cast to `float32` before loss computation to keep the loss path stable.
