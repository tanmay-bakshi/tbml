## Base Model (train.py)

This section documents the text "base model" trained by `tbml/experiments/honeycomb/text/train.py`.
It summarizes the data format, preprocessing, model architecture, losses, and optimization as
implemented in the current code.

### Data format and batching

- Training data is pre-tokenized and stored as fixed-length NumPy shards (`shard_*.npy`) with
  shape `(num_samples, max_seq_len)`. The dataset is loaded via `MMapTokenDataset`, which memory
  maps shards and provides random access.
- Each sample is already padded to `max_seq_len` with a padding token. The dataset metadata
  includes `vocab_size`, `max_seq_len`, and the special token ids (`eos_id`, `pad_id`, `mask_id`).
- Batches are produced by `iter_text_batches`, which performs a streaming shuffle using a
  bounded buffer. The training script reshapes each host batch into per-device slices and
  prefetches to device.
- EOS tokens are stripped before the model forward pass by replacing EOS ids with the padding
  id. This means the model never attends to EOS positions during training.

### View generation and masking

- LeJEPA-style training uses multiple masked "views" per sample: a configurable number of
  **global** and **local** views, plus a single unmasked **sample view**.
- Each view is generated by BART-style span masking:
  - A target masking ratio is sampled uniformly between a per-view min/max (defaults: global
    25–30%, local 40–45%).
  - Span lengths are sampled from a Poisson distribution (lambda = 3).
  - Spans are chosen from non-pad, non-EOS positions.
  - At least one unmasked token is guaranteed per view.
- The masked spans are **not removed** and tokens are **not replaced** with a mask token in the
  base model. Instead, masking is enforced via the attention mask:
  - Masked positions are excluded as keys/values in attention.
  - Outputs at masked positions are zeroed by the attention implementation.
  - Positional indices are preserved, so the model can still infer relative distance.
- The sample view uses the full (unmasked) sequence; it is not used in the LeJEPA loss directly,
  but is used as a target for the predictor loss.

### Model architecture

The base encoder is `TextTransformer` (see `tbml/experiments/honeycomb/text/model.py`).

- **Token embedding**
  - Embedding table of shape `(vocab_size, d_model)`.
  - Embedding normalization is enabled in `train.py` (`embed_norm=True`): embeddings are passed
    through RMSNorm immediately after lookup.
  - The embedding module exposes an `unembed()` method that can project embeddings back to
    vocabulary logits using the (optionally normalized) embedding weights.
- **Transformer blocks**
  - Pre-norm RMSNorm.
  - Self-attention with PoPE or RoPE positional encoding (configured via `attn_type`).
  - Attention is **causal** and also respects the provided attention mask (pads and masked spans).
  - MLP: SwiGLU with expansion ratio `mlp_ratio`.
  - DropPath is applied with a linear schedule over depth.
- **Final normalization**
  - A final RMSNorm is applied to all token representations.
- **Pooling**
  - The pooled sequence embedding is the representation of the **last valid position** according
    to the attention mask (i.e., the last non-masked, non-pad token).
- **Predictor**
  - A separate non‑causal transformer stack that operates on token‑level representations
    after the base model’s final norm.
  - Number of predictor layers is configurable independently (`predictor_n_layers`); other
    hyperparameters (width, heads, MLP ratio, dropout, positional encoding) mirror the base model.
  - Each masked token position is replaced by a learned position‑specific embedding before
    entering the predictor.
  - The predictor outputs token representations **prior** to its own final RMSNorm; the
    final norm is applied only when forming the predictor reconstruction targets.

The model returns both per-token representations and the pooled representation on every call.

### Losses

Let `B` be batch size, `V` be total number of views, and `K` the embedding dimension.
The model produces a tensor of pooled view embeddings with shape `(B, V, K)`.

1) **Sequence reconstruction loss (`seq_rec_loss`)**

- For each sample, the **global center** is computed as:
  - take the pooled *pre‑final‑norm* representations of the global views,
  - average them,
  - then apply the base model’s final RMSNorm to that average.
- All views are compared to this center.
- The prediction loss is either:
  - Mean squared error (MSE), or
  - Cosine distance (1 − cosine similarity).

2) **Sequence SIGReg (`seq_sigreg_loss`)**

- A regularization term computed from random projections of the view embeddings
  (see `tbml/experiments/honeycomb/loss.py`).
- The random directions are synchronized across devices via the global step and seed.
- SIGReg is computed on the **post‑final‑norm** pooled view embeddings.

3) **Sequence LeJEPA loss (`seq_lejepa_loss`)**

```
seq_lejepa_loss = (1 - sigreg_weight) * seq_rec_loss + sigreg_weight * seq_sigreg_loss
```

4) **Span reconstruction loss (`span_rec_loss`)**

- For each global/local view, every masked span is reconstructed by comparing:
  - the base model’s *pre‑final‑norm* sample view representations averaged over that span
    (then passed through the base model’s final RMSNorm), against
  - the predictor’s output for the same span averaged and passed through the predictor’s
    final RMSNorm.
- Mean squared error is applied over these span averages, then averaged across spans and views.

5) **Span SIGReg (`span_sigreg_loss`)**

- A SIGReg term computed on a batch of one randomly selected masked‑span target per sample.

6) **Span LeJEPA loss (`span_lejepa_loss`)**

```
span_lejepa_loss = (1 - sigreg_weight) * span_rec_loss + sigreg_weight * span_sigreg_loss
```

7) **Total loss (`total_loss`)**

```
total_loss = mean(seq_lejepa_loss, span_lejepa_loss)
```

### Optimization and training loop

- Optimizer: `MuonWithAdamWFallback`.
  - Muon is applied to most parameters.
  - AdamW is used for excluded parameter groups (token embeddings, norms, and PoPE delta).
- Gradient accumulation is supported; micro-batches are stacked and reduced on device.
- Training uses `jax.pmap` across devices, with gradients and losses averaged across the
  `"data"` axis.
- The training loop performs a single full pass through the dataset (one epoch), unless
  `--max-train-steps` truncates it.
- Checkpoints are saved every `--checkpoint-every` steps and include model weights,
  optimizer state, and the run config.

### Precision

- Compute dtype is configurable (`float32`, `bfloat16`, `float16`).
- When using reduced precision, model parameters are stored in `float32`.
- Embeddings are cast to `float32` before loss computation to keep the loss path stable.
