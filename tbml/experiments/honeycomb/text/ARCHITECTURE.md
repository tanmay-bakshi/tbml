## Base Model (train.py)

This section documents the text base model trained by `tbml/experiments/honeycomb/text/train.py`.
It summarizes the data format, preprocessing, model architecture, losses, and optimization as
implemented in the current code.

### Data format and batching

- Training data is pre-tokenized and stored as fixed-length NumPy shards (`shard_*.npy`) with
  shape `(num_samples, max_seq_len)`. The dataset is loaded via `MMapTokenDataset`, which memory
  maps shards and provides random access.
- Each sample is already padded to `max_seq_len` with a padding token. The dataset metadata
  includes `vocab_size`, `max_seq_len`, and the special token ids (`eos_id`, `pad_id`, `mask_id`).
- Batches are produced by `iter_text_batches`, which performs a streaming shuffle using a
  bounded buffer. The training script reshapes each host batch into per-device slices and
  prefetches to device.
- EOS tokens are stripped before the model forward pass by replacing EOS ids with the padding
  id. This means the model never attends to EOS positions during training.

### View generation and masking

- TJepa-style training uses masked **global** and **local** views per sample.
- Each view is generated by span or token masking:
  - A target masking ratio is sampled uniformly between a per-view min/max.
  - In `spans` mode, span lengths are sampled from a Poisson distribution (lambda = 3) and
    spans are placed on non-pad, non-EOS positions.
  - In `tokens` mode, individual tokens are selected uniformly at random.
  - At least one unmasked token is guaranteed per view.
- Masking can be applied in one of two encoder input modes:
  - **Hidden mask** (default): masked positions are excluded from the encoder attention mask.
  - **Mask token input** (`--mask-token-input`): masked positions are replaced with the mask
    token id and remain visible in attention.
- The predictor **always** replaces masked positions with its own learned position-specific
  mask embeddings, regardless of the encoder masking mode.

### Model architecture

The base model is `TextTransformer` (see `tbml/experiments/honeycomb/text/model.py`). It is
composed of an **encoder** and an optional **predictor**.

- **Token embedding**
  - Embedding table of shape `(vocab_size, d_model)`.
  - Embedding normalization is configurable in the model config; current training sets
    `embed_norm=False` (no post-lookup RMSNorm).
  - The embedding module exposes an `unembed()` method that can project embeddings back to
    vocabulary logits using the raw embedding weights and a tied bias.
- **Transformer blocks**
  - Pre-norm RMSNorm.
  - Self-attention with PoPE or RoPE positional encoding (configured via `attn_type`).
  - Attention is **causal** for both encoder and predictor when `--causal-attention true`, and
    **non‑causal** when set to false. Attention also respects the provided attention masks
    (pads and masked spans).
  - MLP: SwiGLU with expansion ratio `mlp_ratio`.
  - DropPath is applied with a linear schedule over depth.
- **Encoder output head**
  - Encoder outputs follow: `[transformer] -> [final RMSNorm] -> [SwiGLU FFN]`.
  - The encoder’s token-level outputs used by training and inference are the **post‑head**
    representations (after the SwiGLU FFN).
- **Pooling**
  - The pooled sequence embedding is the representation of the **last valid position** according
    to the attention mask.
- **Predictor**
  - A separate transformer stack that operates on token‑level representations from the encoder
    output head (post‑head). Causality follows the `--causal-attention` setting.
  - Number of predictor layers is configurable independently (`predictor_n_layers`); other
    hyperparameters (width, heads, MLP ratio, dropout, positional encoding) mirror the encoder.
  - Each masked token position is replaced by a learned position‑specific embedding before
    entering the predictor, regardless of the encoder’s masking mode.
  - Predictor outputs are also post‑head: `[predictor blocks] -> [predictor final RMSNorm] ->
    [predictor SwiGLU FFN]`.
- Predictor is optional: if `predictor_n_layers` is set to 0, it is not instantiated and any
  loss depending on it must be disabled.

The model returns both per-token representations and the pooled representation on every call.

### Losses

Let `B` be batch size, `V` be total number of views (global + local), and `K` the embedding
size. The encoder produces token representations of shape `(B, V, T, K)`.

1) **TJepa reconstruction loss (`tjepa_rec_loss`)**

- The encoder processes all global and local views together.
- Only local views are passed through the predictor.
- For each local view, the predictor output is matched **token‑by‑token** to the encoder output
  from the global view for the same sample.
- Masked **and** unmasked tokens are included; EOS and padding positions are excluded.

2) **TJepa SIGReg (`tjepa_sigreg_loss`)**

- For each view, a view‑level representation is computed by mean‑pooling the encoder’s token
  outputs over the attention mask.
  - If masked tokens are hidden from the encoder, they are excluded from the mean.
  - If masked tokens are visible mask‑token inputs, they are included in the mean.
- SIGReg is computed over the `(B, V, K)` tensor of view‑level representations.

3) **TJepa loss (`tjepa_loss`)**

```
tjepa_loss = (1 - sigreg_weight) * tjepa_rec_loss + sigreg_weight * tjepa_sigreg_loss
```

4) **Encoder MLM loss (`encoder_mlm_loss`)**

- For every masked position in every global/local view, the encoder’s post‑head token
  representation is unembedded through the tied token embedding weights.
- Cross‑entropy is computed against the original token id.
- Top‑1/top‑5 accuracy are reported for **masked positions only**.
- If `encoder_mlm_keep_prob > 0`, a random fraction of unmasked tokens also contribute to the
  loss (but still do not affect the accuracy metrics).

5) **Total loss (`total_loss`)**

```
total_loss = tjepa_frac * tjepa_loss + mlm_frac * encoder_mlm_loss
```

The fractions are derived from the CLI flags `--tjepa-loss-weight` and
`--encoder-mlm-loss-weight` by normalizing them to sum to 1.0. When a weight is 0, that loss is
skipped entirely.

### Optimization and training loop

- Optimizer: `MuonWithAdamWFallback`.
  - Muon is applied to most parameters.
  - AdamW is used for excluded parameter groups (token embeddings, norms, and PoPE delta).
- Gradient accumulation is supported; micro-batches are stacked and reduced on device.
- Training uses `jax.pmap` across devices, with gradients and losses averaged across the
  `"data"` axis.
- The training loop performs a single full pass through the dataset (one epoch), unless
  `--max-train-steps` truncates it.
- Checkpoints are saved every `--checkpoint-every` steps and include model weights,
  optimizer state, and the run config.

### Precision

- Compute dtype is configurable (`float32`, `bfloat16`, `float16`).
- When using reduced precision, model parameters are stored in `float32`.
- Embeddings are cast to `float32` before loss computation to keep the loss path stable.
